{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moderation Model Training\n",
    "\n",
    "This notebook demonstrates out-of-core learning for large datasets using PyIceberg and Scikit-Learn.\n",
    "\n",
    "**Scenarios:**\n",
    "1.  **Normal Load:** Fits in memory.\n",
    "2.  **Memory Constraint:** REAL failure (OOM) with large dataset.\n",
    "3.  **Streaming:** Incremental learning solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import joblib\n",
    "import pyarrow as pa\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Schema Definition\n",
    "numeric_features = [\n",
    "    'time_since_upload_seconds', 'hour_of_day', 'day_of_week', \n",
    "    'views_5min', 'views_1hr', 'comments_5min', 'comments_1hr',\n",
    "    'view_velocity_per_min', 'comment_to_view_ratio', 'recent_engagement_score',\n",
    "    'caption_length', 'user_image_count', 'user_age_days'\n",
    "]\n",
    "categorical_features = ['is_weekend', 'has_caption', 'category']\n",
    "label_column = 'label_needs_moderation_24h'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Normal Load (Success)\n",
    "Loads the **Regular** dataset (fits in RAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_1_normal_load():\n",
    "    print(\"--- Scenario 1: In-Memory Training (Small Data) ---\")\n",
    "    try:\n",
    "        catalog = load_catalog(\"gourmetgram\")\n",
    "        # Load normal sized table\n",
    "        table = catalog.load_table(\"moderation.training_data\")\n",
    "        \n",
    "        # Load all data\n",
    "        df = table.scan().to_pandas()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"Dataset empty.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Loaded {len(df)} rows. RAM: {df.memory_usage().sum() / 1e6:.2f} MB\")\n",
    "        \n",
    "        # preprocessing omitted for brevity\n",
    "        X = df[numeric_features].fillna(0)\n",
    "        y = df[label_column]\n",
    "        \n",
    "        model = LogisticRegression()\n",
    "        model.fit(X, y)\n",
    "        print(\"Training complete.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {e}\")\n",
    "\n",
    "scenario_1_normal_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory Constraint (Failure)\n",
    "Attempts to load the **Large** dataset (6M+ rows). Will CRASH if RAM < 2GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_2_memory_crash_simulation():\n",
    "    print(\"--- Scenario 2: Massive Load (Real OOM) ---\")\n",
    "    try:\n",
    "        catalog = load_catalog(\"gourmetgram\")\n",
    "        # Load masssive table\n",
    "        table = catalog.load_table(\"moderation.large_training_data\")\n",
    "        \n",
    "        print(\"Attempting to load HUGE dataset into memory...\")\n",
    "        # THIS WILL CRASH THE KERNEL if memory is limited\n",
    "        df = table.scan().to_pandas()\n",
    "        \n",
    "        print(f\"Loaded {len(df)} rows. RAM: {df.memory_usage().sum() / 1e6:.2f} MB\")\n",
    "        \n",
    "        # Note: We won't reach here if OOM kills the process\n",
    "        print(\"Warning: Dataset fit in memory (Upgrade container limits to see crash)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n CRITICAL ERROR: {e}\")\n",
    "\n",
    "scenario_2_memory_crash_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "The kernel will say that it will restart. In this case we have to run the first cell again before we can run the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Streaming Solution (Success)\n",
    "Uses `SGDClassifier` and `ArrowBatchReader` to train incrementally on the **Large** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def scenario_3_streaming_solution():\n    print(\"--- Scenario 3: Streaming Training (Large Data) ---\")\n    try:\n        catalog = load_catalog(\"gourmetgram\")\n        # Load massive table\n        table = catalog.load_table(\"moderation.large_training_data\")\n        \n        # Initialize incremental model\n        model = SGDClassifier(loss='log_loss', random_state=42)\n        classes = np.array([0, 1])\n        \n        # Stream batches â€” never loads full dataset into memory\n        scanner = table.scan()\n        batches = scanner.to_arrow_batch_reader()\n\n        batch_count = 0\n        total_rows = 0\n        scaler = StandardScaler()\n        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n        first_batch = True\n        \n        for batch in batches:\n            df_batch = batch.to_pandas()\n            if df_batch.empty: continue\n\n            X_batch = df_batch[numeric_features + categorical_features]\n            y_batch = df_batch[label_column]\n\n            # Preprocessing\n            X_num = X_batch[numeric_features].fillna(0)\n            X_cat = X_batch[categorical_features].fillna('Unknown').astype(str)\n\n            if first_batch:\n                scaler.fit(X_num)\n                ohe.fit(X_cat)\n                first_batch = False\n\n            X_transformed = np.hstack((\n                scaler.transform(X_num),\n                ohe.transform(X_cat)\n            ))\n\n            # Partial Fit\n            model.partial_fit(X_transformed, y_batch, classes=classes)\n            \n            batch_count += 1\n            total_rows += len(df_batch)\n            if batch_count % 10 == 0:\n                print(f\"Batch {batch_count}: {len(df_batch)} rows. RAM stable.\")\n                \n        print(f\"\\n SUCCESS: {total_rows} rows trained on LARGE dataset.\")\n        \n        # Save Artifacts\n        os.makedirs(\"models\", exist_ok=True)\n        joblib.dump(model, \"models/moderation_model_demo.joblib\")\n        joblib.dump(scaler, \"models/scaler_demo.joblib\")\n        joblib.dump(ohe, \"models/encoder_demo.joblib\")\n        print(\"Artifacts saved.\")\n        \n    except Exception as e:\n        print(f\"Failed: {e}\")\n\nscenario_3_streaming_solution()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}