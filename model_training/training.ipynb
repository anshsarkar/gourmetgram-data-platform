{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moderation Model Training\n",
    "\n",
    "This notebook demonstrates out-of-core learning for large datasets using PyIceberg and Scikit-Learn.\n",
    "\n",
    "**Scenarios:**\n",
    "1.  **Normal Load:** Fits in memory.\n",
    "2.  **Memory Constraint:** REAL failure (OOM) with large dataset.\n",
    "3.  **Streaming:** Incremental learning solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import joblib\n",
    "import pyarrow as pa\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Schema Definition\n",
    "numeric_features = [\n",
    "    'time_since_upload_seconds', 'hour_of_day', 'day_of_week', \n",
    "    'views_5min', 'views_1hr', 'comments_5min', 'comments_1hr',\n",
    "    'view_velocity_per_min', 'comment_to_view_ratio', 'recent_engagement_score',\n",
    "    'caption_length', 'user_image_count', 'user_age_days'\n",
    "]\n",
    "categorical_features = ['is_weekend', 'has_caption', 'category']\n",
    "label_column = 'label_needs_moderation_24h'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Normal Load (Success)\n",
    "Loads the **Regular** dataset (fits in RAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_1_normal_load():\n",
    "    print(\"--- Scenario 1: In-Memory Training (Small Data) ---\")\n",
    "    try:\n",
    "        catalog = load_catalog(\"gourmetgram\")\n",
    "        # Load normal sized table\n",
    "        table = catalog.load_table(\"moderation.training_data\")\n",
    "        \n",
    "        # Load all data\n",
    "        df = table.scan().to_pandas()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"Dataset empty.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Loaded {len(df)} rows. RAM: {df.memory_usage().sum() / 1e6:.2f} MB\")\n",
    "        \n",
    "        # preprocessing omitted for brevity\n",
    "        X = df[numeric_features].fillna(0)\n",
    "        y = df[label_column]\n",
    "        \n",
    "        model = LogisticRegression()\n",
    "        model.fit(X, y)\n",
    "        print(\"Training complete.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {e}\")\n",
    "\n",
    "scenario_1_normal_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory Constraint (Failure)\n",
    "Attempts to load the **Large** dataset (6M+ rows). Will CRASH if RAM < 2GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_2_memory_crash_simulation():\n",
    "    print(\"--- Scenario 2: Massive Load (Real OOM) ---\")\n",
    "    try:\n",
    "        catalog = load_catalog(\"gourmetgram\")\n",
    "        # Load masssive table\n",
    "        table = catalog.load_table(\"moderation.large_training_data\")\n",
    "        \n",
    "        print(\"Attempting to load HUGE dataset into memory...\")\n",
    "        # THIS WILL CRASH THE KERNEL if memory is limited\n",
    "        df = table.scan().to_pandas()\n",
    "        \n",
    "        print(f\"Loaded {len(df)} rows. RAM: {df.memory_usage().sum() / 1e6:.2f} MB\")\n",
    "        \n",
    "        # Note: We won't reach here if OOM kills the process\n",
    "        print(\"Warning: Dataset fit in memory (Upgrade container limits to see crash)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n CRITICAL ERROR: {e}\")\n",
    "\n",
    "scenario_2_memory_crash_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Streaming Solution (Success)\n",
    "Uses `SGDClassifier` and `ArrowBatchReader` to train incrementally on the **Large** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_3_streaming_solution():\n",
    "    print(\"--- Scenario 3: Streaming Training (Large Data) ---\")\n",
    "    try:\n",
    "        catalog = load_catalog(\"gourmetgram\")\n",
    "        # Load massive table\n",
    "        table = catalog.load_table(\"moderation.training_data\")\n",
    "        \n",
    "        # Initialize incremental model\n",
    "        model = SGDClassifier(loss='log_loss', random_state=42)\n",
    "        classes = np.array([0, 1])\n",
    "        \n",
    "        # Stream batches\n",
    "        scanner = table.scan()\n",
    "        try:\n",
    "            batches = scanner.to_arrow_batch_reader()\n",
    "        except Exception:\n",
    "             # Fallback for compatibility\n",
    "             full_df = scanner.to_pandas()\n",
    "             batches = [pa.Table.from_pandas(full_df)]\n",
    "\n",
    "        batch_count = 0\n",
    "        total_rows = 0\n",
    "        scaler = StandardScaler()\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        first_batch = True\n",
    "        \n",
    "        for batch in batches:\n",
    "            df_batch = batch.to_pandas()\n",
    "            if df_batch.empty: continue\n",
    "\n",
    "            X_batch = df_batch[numeric_features + categorical_features]\n",
    "            y_batch = df_batch[label_column]\n",
    "\n",
    "            # Preprocessing\n",
    "            X_num = X_batch[numeric_features].fillna(0)\n",
    "            X_cat = X_batch[categorical_features].fillna('Unknown').astype(str)\n",
    "\n",
    "            if first_batch:\n",
    "                scaler.fit(X_num)\n",
    "                ohe.fit(X_cat)\n",
    "                first_batch = False\n",
    "\n",
    "            X_transformed = np.hstack((\n",
    "                scaler.transform(X_num),\n",
    "                ohe.transform(X_cat)\n",
    "            ))\n",
    "\n",
    "            # Partial Fit\n",
    "            model.partial_fit(X_transformed, y_batch, classes=classes)\n",
    "            \n",
    "            batch_count += 1\n",
    "            total_rows += len(df_batch)\n",
    "            if batch_count % 10 == 0:\n",
    "                print(f\"Batch {batch_count}: {len(df_batch)} rows. RAM stable.\")\n",
    "                \n",
    "        print(f\"\\n SUCCESS: {total_rows} rows trained on LARGE dataset.\")\n",
    "        \n",
    "        # Save Artifacts\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        joblib.dump(model, \"models/moderation_model.joblib\")\n",
    "        joblib.dump(scaler, \"models/scaler.joblib\")\n",
    "        joblib.dump(ohe, \"models/encoder.joblib\")\n",
    "        print(\"Artifacts saved.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {e}\")\n",
    "\n",
    "scenario_3_streaming_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload Model to MinIO\n",
    "\n",
    "Upload the trained model artifacts to MinIO so the **inference service** can access them.\n",
    "After running this cell, the inference service will automatically detect the new model within 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=os.getenv(\"S3_ENDPOINT_URL\", \"http://minio:9000\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\", \"admin\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"password\"),\n",
    ")\n",
    "\n",
    "bucket = \"gourmetgram-datalake\"\n",
    "prefix = \"models/moderation/\"\n",
    "\n",
    "# Upload model artifacts\n",
    "file_mapping = {\n",
    "    \"models/moderation_model.joblib\": \"model.joblib\",\n",
    "    \"models/scaler.joblib\": \"scaler.joblib\",\n",
    "    \"models/encoder.joblib\": \"encoder.joblib\",\n",
    "}\n",
    "\n",
    "for local_path, remote_name in file_mapping.items():\n",
    "    s3.upload_file(local_path, bucket, f\"{prefix}{remote_name}\")\n",
    "    print(f\"Uploaded {local_path} -> s3://{bucket}/{prefix}{remote_name}\")\n",
    "\n",
    "# Upload metadata (inference service watches this to detect new models)\n",
    "metadata = {\n",
    "    \"version\": 1,\n",
    "    \"trained_at\": datetime.utcnow().isoformat(),\n",
    "    \"updated_at\": datetime.utcnow().isoformat(),\n",
    "    \"algorithm\": \"SGDClassifier\",\n",
    "}\n",
    "s3.put_object(\n",
    "    Bucket=bucket,\n",
    "    Key=f\"{prefix}metadata.json\",\n",
    "    Body=json.dumps(metadata),\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "print(f\"\\nUploaded metadata.json -> s3://{bucket}/{prefix}metadata.json\")\n",
    "print(\"\\nModel artifacts uploaded! The inference service will detect them within 30 seconds.\")\n",
    "print(\"To use the model: edit inference_service/moderator.py -> set USE_MODEL = True -> restart the service.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
